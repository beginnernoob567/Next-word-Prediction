{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d07105da",
   "metadata": {},
   "source": [
    "Importing the necessary files.\n",
    "\n",
    "Tokenizer is used to create tokens of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b433edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding,LSTM,Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0cac55",
   "metadata": {},
   "source": [
    "We open our dataset file and append the lines one by one into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f9ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"C:/Krish/Programming/Machine Learning/Files/Data/metamorphosis_clean.txt\",\"r\",encoding=\"utf8\")\n",
    "lines=[]\n",
    "for i in file:\n",
    "    lines.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41a24130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The First Line:  ï»¿One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "\n",
      "The Last Line:  himself transformed in his bed into a horrible vermin.  He lay on\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The First Line: \",lines[0])\n",
    "print(\"The Last Line: \",lines[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e936287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffOne morning, when Gregor Samsa woke from troubled dreams, he found\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89a671ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'himself transformed in his bed into a horrible vermin.  He lay on\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb5c351",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8749a",
   "metadata": {},
   "source": [
    "Here we clean our texts by replacing the unnecessary characters like '\\n','\\r' with a space.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86101deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffOne morning, when Gregor Samsa woke from troubled dreams, he found\\n',\n",
       " 'himself transformed in his bed into a horrible vermin.  He lay on\\n',\n",
       " 'his armour-like back, and if he lifted his head a little he could\\n',\n",
       " 'see his brown belly, slightly domed and divided by arches into stiff\\n',\n",
       " 'sections.  The bedding was hardly able to cover it and seemed ready\\n',\n",
       " 'to slide off any moment.  His many legs, pitifully thin compared\\n',\n",
       " 'with the size of the rest of him, waved about helplessly as he\\n',\n",
       " 'looked.\\n',\n",
       " '\\n',\n",
       " '\"What\\'s happened to me?\" he thought.  It wasn\\'t a dream.  His room,\\n']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb4d3f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin.  He lay on his armour-like back, and if he lifted his head a little he could see his brown belly, slightly domed and divided by arches into stiff sections.  The bedding was hardly able to cover it and seemed ready to slide off any moment.  His many legs, pitifully thin compared with the size of the rest of him, waved about helplessly as he looked.  \"What\\'s happened to me?\" he'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=\"\"\n",
    "\n",
    "for i in lines:\n",
    "    data=' '.join(lines)\n",
    "    \n",
    "data=data.replace('\\n','').replace('\\r','').replace('\\ufeff','')\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9eaced",
   "metadata": {},
   "source": [
    "### maketrans()\n",
    "maketrans()- it isused to replace a character from a set of strings.\n",
    "maketrans(str1,str2,str3)- str1 the character that needs to be replaced,str2 the character that should be placed, str3 the character that should be deleted.\n",
    "the output for this would be in ascii number. For eg, maketrans('a','b') will give the output {97:98}.\n",
    "\n",
    "### translate()\n",
    "translate() is used to convert those ascii values into characters and make the necessary changes. Here a will be replaced with b after we use the translate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09b02a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{97: 98}\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "strr='a'\n",
    "\n",
    "translate=str.maketrans('a','b')\n",
    "strr=strr.translate(translate)\n",
    "\n",
    "print(translate)\n",
    "print(strr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a7ae6",
   "metadata": {},
   "source": [
    "Here we convert all the punctuation marks into a space character. by mapping the ascii values of the punctuaion marks with the ascii value of the space character and then using the translate function to convert the values to characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e5bca15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning  when Gregor Samsa woke from troubled dreams  he found himself transformed in his bed into a horrible vermin   He lay on his armour like back  and if he lifted his head a little he could see his brown belly  slightly domed and divided by arches into stiff sections   The bedding was hardly able to cover it and seemed ready to slide off any moment   His many legs  pitifully thin compared with the size of the rest of him  waved about helplessly as he looked    What s happened to me   he'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "translator=str.maketrans(string.punctuation,' '*len(string.punctuation))\n",
    "new_data=data.translate(translator)\n",
    "\n",
    "new_data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f88a2ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ascii character of space(' ')- 32\n",
      "Mapped punctuation characters: \n",
      "{33: 32, 34: 32, 35: 32, 36: 32, 37: 32, 38: 32, 39: 32, 40: 32, 41: 32, 42: 32, 43: 32, 44: 32, 45: 32, 46: 32, 47: 32, 58: 32, 59: 32, 60: 32, 61: 32, 62: 32, 63: 32, 64: 32, 91: 32, 92: 32, 93: 32, 94: 32, 95: 32, 96: 32, 123: 32, 124: 32, 125: 32, 126: 32}\n"
     ]
    }
   ],
   "source": [
    "print(\"Ascii character of space(' ')-\",ord(' '))\n",
    "print(\"Mapped punctuation characters: \")\n",
    "print(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb764d6",
   "metadata": {},
   "source": [
    "Here we remove all the words that have been repeated more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28f07313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One morning, when Gregor Samsa woke from troubled dreams, he found himself transformed in his bed into a horrible vermin. He lay on armour-like back, and if lifted head little could see brown belly, slightly domed divided by arches stiff sections. The bedding was hardly able to cover it seemed ready slide off any moment. His many legs, pitifully thin compared with the size of rest him, waved about helplessly as looked. \"What\\'s happened me?\" thought. It wasn\\'t dream. room, proper human room altho'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z=[]\n",
    "\n",
    "for i in data.split():\n",
    "    if i  not in z:\n",
    "        z.append(i)\n",
    "\n",
    "data=' '.join(z)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9f886dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiiamaprogrammer\n",
      "Hi i am a programmer\n"
     ]
    }
   ],
   "source": [
    "sentence='Hi i am a programmer'\n",
    "\n",
    "sentence1=sentence.split()\n",
    "sentence2=''.join(sentence1)\n",
    "print(sentence2)\n",
    "sentence=' '.join(sentence1)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8889b0",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367974c1",
   "metadata": {},
   "source": [
    "We create tokens of all the words, i.e we assign a value to each word "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af0ceac",
   "metadata": {},
   "source": [
    "fit_on_texts: This method is used to update the internal vocabulary of the tokenizer based on the provided text\n",
    "data. It takes a list of texts as input and updates the tokenizer's internal state to include the words present in the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79471146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 139, 55, 14, 93, 935, 28, 1313, 936, 4]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "pickle.dump(tokenizer,open('tokenizer1.pkl','wb'))\n",
    "\n",
    "sequence_data=tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d64505d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'to': 2,\n",
       " 'and': 3,\n",
       " 'he': 4,\n",
       " 'his': 5,\n",
       " 'of': 6,\n",
       " 'was': 7,\n",
       " 'it': 8,\n",
       " 'had': 9,\n",
       " 'in': 10,\n",
       " 'that': 11,\n",
       " 'a': 12,\n",
       " 'as': 13,\n",
       " 'gregor': 14,\n",
       " 'with': 15,\n",
       " 'she': 16,\n",
       " 'him': 17,\n",
       " 'her': 18,\n",
       " 'would': 19,\n",
       " 'not': 20,\n",
       " 'but': 21,\n",
       " 'at': 22,\n",
       " 'for': 23,\n",
       " 'they': 24,\n",
       " 'on': 25,\n",
       " 'all': 26,\n",
       " 'room': 27,\n",
       " 'from': 28,\n",
       " 'could': 29,\n",
       " 'be': 30,\n",
       " 'out': 31,\n",
       " 'have': 32,\n",
       " 'if': 33,\n",
       " 'there': 34,\n",
       " 'been': 35,\n",
       " \"gregor's\": 36,\n",
       " 'so': 37,\n",
       " 'father': 38,\n",
       " 'sister': 39,\n",
       " 'this': 40,\n",
       " 'now': 41,\n",
       " 'himself': 42,\n",
       " 'door': 43,\n",
       " 'then': 44,\n",
       " 'back': 45,\n",
       " 'mother': 46,\n",
       " 'up': 47,\n",
       " 'even': 48,\n",
       " 'into': 49,\n",
       " 'no': 50,\n",
       " 'did': 51,\n",
       " 'one': 52,\n",
       " 'more': 53,\n",
       " 'their': 54,\n",
       " 'when': 55,\n",
       " 'were': 56,\n",
       " 'what': 57,\n",
       " 'about': 58,\n",
       " 'them': 59,\n",
       " 'way': 60,\n",
       " 'only': 61,\n",
       " 'time': 62,\n",
       " 'i': 63,\n",
       " 'by': 64,\n",
       " 'than': 65,\n",
       " 'you': 66,\n",
       " 'just': 67,\n",
       " 'said': 68,\n",
       " 'little': 69,\n",
       " 'any': 70,\n",
       " 'do': 71,\n",
       " 'get': 72,\n",
       " 'other': 73,\n",
       " 'still': 74,\n",
       " 'first': 75,\n",
       " 'or': 76,\n",
       " 'made': 77,\n",
       " 'go': 78,\n",
       " 'some': 79,\n",
       " 'while': 80,\n",
       " 'see': 81,\n",
       " 'again': 82,\n",
       " 'without': 83,\n",
       " 'like': 84,\n",
       " 'head': 85,\n",
       " 'before': 86,\n",
       " 'much': 87,\n",
       " 'after': 88,\n",
       " 'where': 89,\n",
       " 'chief': 90,\n",
       " 'down': 91,\n",
       " 'open': 92,\n",
       " 'samsa': 93,\n",
       " 'which': 94,\n",
       " 'very': 95,\n",
       " 'clerk': 96,\n",
       " 'who': 97,\n",
       " 'thought': 98,\n",
       " 'well': 99,\n",
       " 'over': 100,\n",
       " 'went': 101,\n",
       " 'come': 102,\n",
       " 'away': 103,\n",
       " 'left': 104,\n",
       " 'an': 105,\n",
       " 'soon': 106,\n",
       " 'family': 107,\n",
       " 'came': 108,\n",
       " 'too': 109,\n",
       " 'quite': 110,\n",
       " 'though': 111,\n",
       " 'bed': 112,\n",
       " 'looked': 113,\n",
       " 'how': 114,\n",
       " 'day': 115,\n",
       " 'everything': 116,\n",
       " 'wanted': 117,\n",
       " 'something': 118,\n",
       " 'let': 119,\n",
       " 'against': 120,\n",
       " 'two': 121,\n",
       " 'seemed': 122,\n",
       " 'is': 123,\n",
       " 'being': 124,\n",
       " 'work': 125,\n",
       " 'we': 126,\n",
       " 'parents': 127,\n",
       " 'round': 128,\n",
       " 'three': 129,\n",
       " 'gentlemen': 130,\n",
       " 'body': 131,\n",
       " 'things': 132,\n",
       " 'able': 133,\n",
       " 'because': 134,\n",
       " 'make': 135,\n",
       " 'already': 136,\n",
       " 'really': 137,\n",
       " 'mr': 138,\n",
       " 'morning': 139,\n",
       " 'look': 140,\n",
       " 'slowly': 141,\n",
       " 'long': 142,\n",
       " 'side': 143,\n",
       " 'floor': 144,\n",
       " 'hand': 145,\n",
       " 'grete': 146,\n",
       " 'onto': 147,\n",
       " 'got': 148,\n",
       " 'my': 149,\n",
       " 'legs': 150,\n",
       " 'used': 151,\n",
       " 'never': 152,\n",
       " 'better': 153,\n",
       " 'enough': 154,\n",
       " 'think': 155,\n",
       " 'through': 156,\n",
       " 'flat': 157,\n",
       " 'around': 158,\n",
       " 'lay': 159,\n",
       " 'off': 160,\n",
       " 'although': 161,\n",
       " 'table': 162,\n",
       " 'turned': 163,\n",
       " 'window': 164,\n",
       " 'heard': 165,\n",
       " 'feel': 166,\n",
       " 'right': 167,\n",
       " 'eyes': 168,\n",
       " 'home': 169,\n",
       " 'become': 170,\n",
       " 'hands': 171,\n",
       " 'under': 172,\n",
       " 'here': 173,\n",
       " 'good': 174,\n",
       " 'its': 175,\n",
       " 'whole': 176,\n",
       " 'longer': 177,\n",
       " 'always': 178,\n",
       " 'must': 179,\n",
       " 'began': 180,\n",
       " 'once': 181,\n",
       " 'possible': 182,\n",
       " 'next': 183,\n",
       " 'perhaps': 184,\n",
       " 'nothing': 185,\n",
       " 'almost': 186,\n",
       " 'took': 187,\n",
       " 'say': 188,\n",
       " 'evening': 189,\n",
       " 'straight': 190,\n",
       " 'living': 191,\n",
       " 'herself': 192,\n",
       " 'me': 193,\n",
       " 'know': 194,\n",
       " 'should': 195,\n",
       " 'voice': 196,\n",
       " 'each': 197,\n",
       " 'might': 198,\n",
       " 'front': 199,\n",
       " 'couch': 200,\n",
       " 'hardly': 201,\n",
       " 'your': 202,\n",
       " \"it's\": 203,\n",
       " 'move': 204,\n",
       " 'same': 205,\n",
       " 'help': 206,\n",
       " 'opened': 207,\n",
       " 'immediately': 208,\n",
       " 'asked': 209,\n",
       " 'chair': 210,\n",
       " 'behind': 211,\n",
       " 'happened': 212,\n",
       " 'times': 213,\n",
       " 'felt': 214,\n",
       " 'business': 215,\n",
       " 'can': 216,\n",
       " 'furniture': 217,\n",
       " 'probably': 218,\n",
       " \"i'm\": 219,\n",
       " 'often': 220,\n",
       " 'became': 221,\n",
       " 'course': 222,\n",
       " 'also': 223,\n",
       " 'women': 224,\n",
       " 'taken': 225,\n",
       " 'sat': 226,\n",
       " 'sleep': 227,\n",
       " 'doing': 228,\n",
       " 'food': 229,\n",
       " 'these': 230,\n",
       " 'chest': 231,\n",
       " 'put': 232,\n",
       " 'called': 233,\n",
       " 'anything': 234,\n",
       " 'arms': 235,\n",
       " 'held': 236,\n",
       " 'stood': 237,\n",
       " 'attention': 238,\n",
       " 'pressed': 239,\n",
       " 'face': 240,\n",
       " 'found': 241,\n",
       " 'life': 242,\n",
       " 'close': 243,\n",
       " 'money': 244,\n",
       " 'certainly': 245,\n",
       " 'seen': 246,\n",
       " 'hear': 247,\n",
       " 'until': 248,\n",
       " 'playing': 249,\n",
       " 'towards': 250,\n",
       " 'hard': 251,\n",
       " 'tried': 252,\n",
       " 'pushed': 253,\n",
       " 'saw': 254,\n",
       " 'quickly': 255,\n",
       " 'especially': 256,\n",
       " 'later': 257,\n",
       " 'ever': 258,\n",
       " 'words': 259,\n",
       " 'done': 260,\n",
       " \"don't\": 261,\n",
       " 'forward': 262,\n",
       " 'new': 263,\n",
       " 'every': 264,\n",
       " 'leave': 265,\n",
       " 'reason': 266,\n",
       " 'understand': 267,\n",
       " 'take': 268,\n",
       " 'old': 269,\n",
       " 'violin': 270,\n",
       " 'pain': 271,\n",
       " 'making': 272,\n",
       " \"didn't\": 273,\n",
       " 'thing': 274,\n",
       " \"that's\": 275,\n",
       " 'moving': 276,\n",
       " 'set': 277,\n",
       " 'want': 278,\n",
       " 'give': 279,\n",
       " 'most': 280,\n",
       " 'keep': 281,\n",
       " 'last': 282,\n",
       " 'stay': 283,\n",
       " 'moved': 284,\n",
       " 'remained': 285,\n",
       " 'least': 286,\n",
       " 'us': 287,\n",
       " 'kitchen': 288,\n",
       " 'clear': 289,\n",
       " 'ran': 290,\n",
       " 'key': 291,\n",
       " 'eat': 292,\n",
       " 'middle': 293,\n",
       " 'moment': 294,\n",
       " \"what's\": 295,\n",
       " 'state': 296,\n",
       " 'however': 297,\n",
       " 'effort': 298,\n",
       " 'own': 299,\n",
       " 'whether': 300,\n",
       " 'doors': 301,\n",
       " 'night': 302,\n",
       " 'themselves': 303,\n",
       " 'finally': 304,\n",
       " 'despite': 305,\n",
       " 'will': 306,\n",
       " 'quiet': 307,\n",
       " 'mrs': 308,\n",
       " 'few': 309,\n",
       " 'great': 310,\n",
       " 'turn': 311,\n",
       " 'looking': 312,\n",
       " 'upright': 313,\n",
       " 'covered': 314,\n",
       " 'present': 315,\n",
       " 'shut': 316,\n",
       " 'stopped': 317,\n",
       " 'train': 318,\n",
       " 'boss': 319,\n",
       " 'told': 320,\n",
       " 'desk': 321,\n",
       " 'together': 322,\n",
       " 'another': 323,\n",
       " 'drawers': 324,\n",
       " 'half': 325,\n",
       " 'seven': 326,\n",
       " 'completely': 327,\n",
       " 'noticed': 328,\n",
       " 'carefully': 329,\n",
       " 'dressed': 330,\n",
       " 'fell': 331,\n",
       " \"can't\": 332,\n",
       " 'clearly': 333,\n",
       " 'use': 334,\n",
       " 'far': 335,\n",
       " 'why': 336,\n",
       " 'suddenly': 337,\n",
       " 'since': 338,\n",
       " 'tears': 339,\n",
       " 'realised': 340,\n",
       " 'taking': 341,\n",
       " 'going': 342,\n",
       " 'charwoman': 343,\n",
       " 'cleaner': 344,\n",
       " 'slightly': 345,\n",
       " 'travelling': 346,\n",
       " \"couldn't\": 347,\n",
       " 'position': 348,\n",
       " 'anyone': 349,\n",
       " 'place': 350,\n",
       " 'eating': 351,\n",
       " 'notice': 352,\n",
       " 'fall': 353,\n",
       " \"i'll\": 354,\n",
       " 'yes': 355,\n",
       " 'noise': 356,\n",
       " 'having': 357,\n",
       " 'word': 358,\n",
       " 'peace': 359,\n",
       " 'bring': 360,\n",
       " 'sign': 361,\n",
       " 'others': 362,\n",
       " 'force': 363,\n",
       " 'kept': 364,\n",
       " 'loud': 365,\n",
       " 'across': 366,\n",
       " 'needed': 367,\n",
       " 'knew': 368,\n",
       " 'hours': 369,\n",
       " 'speak': 370,\n",
       " 'please': 371,\n",
       " 'else': 372,\n",
       " 'nearly': 373,\n",
       " \"he's\": 374,\n",
       " 'job': 375,\n",
       " 'condition': 376,\n",
       " 'hurried': 377,\n",
       " 'lock': 378,\n",
       " 'mouth': 379,\n",
       " 'leant': 380,\n",
       " 'several': 381,\n",
       " 'uniform': 382,\n",
       " 'finished': 383,\n",
       " 'doorway': 384,\n",
       " 'closed': 385,\n",
       " 'everyone': 386,\n",
       " 'stand': 387,\n",
       " 'rest': 388,\n",
       " 'between': 389,\n",
       " 'walls': 390,\n",
       " 'spread': 391,\n",
       " 'nice': 392,\n",
       " 'lower': 393,\n",
       " 'different': 394,\n",
       " 'white': 395,\n",
       " 'getting': 396,\n",
       " 'during': 397,\n",
       " 'maybe': 398,\n",
       " 'years': 399,\n",
       " \"o'clock\": 400,\n",
       " 'man': 401,\n",
       " 'ill': 402,\n",
       " 'hurriedly': 403,\n",
       " 'near': 404,\n",
       " 'full': 405,\n",
       " 'conversation': 406,\n",
       " 'need': 407,\n",
       " 'both': 408,\n",
       " 'breakfast': 409,\n",
       " 'instead': 410,\n",
       " 'serious': 411,\n",
       " 'difficult': 412,\n",
       " 'part': 413,\n",
       " 'turning': 414,\n",
       " 'carry': 415,\n",
       " 'earlier': 416,\n",
       " 'order': 417,\n",
       " 'calm': 418,\n",
       " 'expected': 419,\n",
       " 'maid': 420,\n",
       " 'sound': 421,\n",
       " 'question': 422,\n",
       " 'simply': 423,\n",
       " 'easy': 424,\n",
       " 'strength': 425,\n",
       " 'knowing': 426,\n",
       " 'sight': 427,\n",
       " \"mother's\": 428,\n",
       " 'hall': 429,\n",
       " 'kind': 430,\n",
       " 'breath': 431,\n",
       " 'ground': 432,\n",
       " 'chance': 433,\n",
       " 'reached': 434,\n",
       " 'pleasure': 435,\n",
       " 'end': 436,\n",
       " 'fully': 437,\n",
       " 'light': 438,\n",
       " 'ceiling': 439,\n",
       " 'actually': 440,\n",
       " 'dish': 441,\n",
       " 'sometimes': 442,\n",
       " 'music': 443,\n",
       " 'totally': 444,\n",
       " 'tired': 445,\n",
       " 'crawl': 446,\n",
       " 'lifted': 447,\n",
       " 'picture': 448,\n",
       " 'forget': 449,\n",
       " 'unable': 450,\n",
       " 'sleeping': 451,\n",
       " 'oh': 452,\n",
       " 'god': 453,\n",
       " 'people': 454,\n",
       " 'early': 455,\n",
       " 'whenever': 456,\n",
       " 'are': 457,\n",
       " 'try': 458,\n",
       " 'given': 459,\n",
       " 'tell': 460,\n",
       " 'sort': 461,\n",
       " 'five': 462,\n",
       " 'alarm': 463,\n",
       " 'past': 464,\n",
       " 'quarter': 465,\n",
       " 'fresh': 466,\n",
       " 'office': 467,\n",
       " 'doctor': 468,\n",
       " 'wrong': 469,\n",
       " 'struck': 470,\n",
       " 'leaving': 471,\n",
       " 'aware': 472,\n",
       " 'habit': 473,\n",
       " 'lying': 474,\n",
       " 'caused': 475,\n",
       " 'slightest': 476,\n",
       " 'free': 477,\n",
       " 'painfully': 478,\n",
       " 'air': 479,\n",
       " 'unfortunately': 480,\n",
       " 'street': 481,\n",
       " 'carpet': 482,\n",
       " 'find': 483,\n",
       " 'someone': 484,\n",
       " 'anyway': 485,\n",
       " 'sure': 486,\n",
       " \"you're\": 487,\n",
       " 'year': 488,\n",
       " 'gave': 489,\n",
       " 'listen': 490,\n",
       " 'listening': 491,\n",
       " 'hold': 492,\n",
       " 'meal': 493,\n",
       " 'landing': 494,\n",
       " 'earn': 495,\n",
       " 'impossible': 496,\n",
       " 'foot': 497,\n",
       " 'rushed': 498,\n",
       " 'lost': 499,\n",
       " 'forgotten': 500,\n",
       " 'running': 501,\n",
       " 'flew': 502,\n",
       " \"father's\": 503,\n",
       " 'milk': 504,\n",
       " 'crawling': 505,\n",
       " 'days': 506,\n",
       " 'bent': 507,\n",
       " 'sheet': 508,\n",
       " 'meant': 509,\n",
       " 'many': 510,\n",
       " \"wasn't\": 511,\n",
       " 'human': 512,\n",
       " 'cut': 513,\n",
       " 'threw': 514,\n",
       " \"wouldn't\": 515,\n",
       " 'friendly': 516,\n",
       " 'overcome': 517,\n",
       " \"i'd\": 518,\n",
       " 'gone': 519,\n",
       " 'pay': 520,\n",
       " 'clock': 521,\n",
       " 'quietly': 522,\n",
       " 'rush': 523,\n",
       " 'fact': 524,\n",
       " 'usual': 525,\n",
       " 'shocked': 526,\n",
       " 'outside': 527,\n",
       " 'short': 528,\n",
       " 'sides': 529,\n",
       " 'throw': 530,\n",
       " 'learned': 531,\n",
       " 'easily': 532,\n",
       " 'followed': 533,\n",
       " 'harder': 534,\n",
       " 'consideration': 535,\n",
       " 'raised': 536,\n",
       " 'happen': 537,\n",
       " 'patient': 538,\n",
       " 'locked': 539,\n",
       " 'steps': 540,\n",
       " 'necessary': 541,\n",
       " 'show': 542,\n",
       " 'stayed': 543,\n",
       " 'idea': 544,\n",
       " 'glad': 545,\n",
       " 'seriously': 546,\n",
       " 'speaking': 547,\n",
       " 'late': 548,\n",
       " 'beside': 549,\n",
       " 'alright': 550,\n",
       " 'wait': 551,\n",
       " 'loudly': 552,\n",
       " 'holding': 553,\n",
       " 'using': 554,\n",
       " 'start': 555,\n",
       " 'disappeared': 556,\n",
       " 'along': 557,\n",
       " 'wall': 558,\n",
       " 'enormous': 559,\n",
       " 'urge': 560,\n",
       " 'spare': 561,\n",
       " 'stick': 562,\n",
       " 'pulled': 563,\n",
       " 'stop': 564,\n",
       " 'allowed': 565,\n",
       " 'intentions': 566,\n",
       " 'slammed': 567,\n",
       " 'filled': 568,\n",
       " 'either': 569,\n",
       " 'forced': 570,\n",
       " \"sister's\": 571,\n",
       " 'surprise': 572,\n",
       " 'carried': 573,\n",
       " 'startled': 574,\n",
       " 'asleep': 575,\n",
       " 'broom': 576,\n",
       " 'send': 577,\n",
       " 'indeed': 578,\n",
       " 'silent': 579,\n",
       " 'slow': 580,\n",
       " 'coming': 581,\n",
       " 'writing': 582,\n",
       " 'immobile': 583,\n",
       " 'shouted': 584,\n",
       " 'apple': 585,\n",
       " 'sit': 586,\n",
       " 'appeared': 587,\n",
       " 'gentleman': 588,\n",
       " 'brown': 589,\n",
       " 'ready': 590,\n",
       " 'small': 591,\n",
       " 'four': 592,\n",
       " 'above': 593,\n",
       " 'hung': 594,\n",
       " 'lady': 595,\n",
       " 'fur': 596,\n",
       " 'hat': 597,\n",
       " 'raising': 598,\n",
       " 'heavy': 599,\n",
       " 'arm': 600,\n",
       " 'bit': 601,\n",
       " \"i've\": 602,\n",
       " \"there's\": 603,\n",
       " 'slight': 604,\n",
       " 'drew': 605,\n",
       " 'touched': 606,\n",
       " 'live': 607,\n",
       " 'ought': 608,\n",
       " 'best': 609,\n",
       " 'talking': 610,\n",
       " 'hope': 611,\n",
       " 'change': 612,\n",
       " 'forwards': 613,\n",
       " 'catch': 614,\n",
       " 'entirely': 615,\n",
       " 'somebody': 616,\n",
       " 'somewhere': 617,\n",
       " 'answer': 618,\n",
       " 'saying': 619,\n",
       " 'gently': 620,\n",
       " 'answered': 621,\n",
       " 'remove': 622,\n",
       " 'disturbed': 623,\n",
       " 'thoughts': 624,\n",
       " 'sensible': 625,\n",
       " 'today': 626,\n",
       " 'push': 627,\n",
       " 'those': 628,\n",
       " 'control': 629,\n",
       " 'itself': 630,\n",
       " 'trying': 631,\n",
       " 'direction': 632,\n",
       " 'hit': 633,\n",
       " 'pushing': 634,\n",
       " 'whatever': 635,\n",
       " 'view': 636,\n",
       " 'confidence': 637,\n",
       " 'entire': 638,\n",
       " 'concern': 639,\n",
       " 'strong': 640,\n",
       " 'mind': 641,\n",
       " 'careful': 642,\n",
       " 'call': 643,\n",
       " 'difficulty': 644,\n",
       " 'smile': 645,\n",
       " 'decision': 646,\n",
       " 'highly': 647,\n",
       " 'employees': 648,\n",
       " 'annoyed': 649,\n",
       " 'fallen': 650,\n",
       " 'has': 651,\n",
       " \"isn't\": 652,\n",
       " 'evenings': 653,\n",
       " 'working': 654,\n",
       " 'amazed': 655,\n",
       " 'worry': 656,\n",
       " 'happening': 657,\n",
       " 'astonished': 658,\n",
       " 'seem': 659,\n",
       " 'animal': 660,\n",
       " 'entrance': 661,\n",
       " 'locksmith': 662,\n",
       " 'skirts': 663,\n",
       " 'situation': 664,\n",
       " 'teeth': 665,\n",
       " 'lack': 666,\n",
       " 'break': 667,\n",
       " 'wide': 668,\n",
       " 'movement': 669,\n",
       " 'important': 670,\n",
       " 'reading': 671,\n",
       " 'exactly': 672,\n",
       " 'respect': 673,\n",
       " 'our': 674,\n",
       " 'nobody': 675,\n",
       " 'started': 676,\n",
       " 'stretched': 677,\n",
       " 'save': 678,\n",
       " 'mood': 679,\n",
       " 'future': 680,\n",
       " 'talk': 681,\n",
       " 'backwards': 682,\n",
       " 'self': 683,\n",
       " 'newspaper': 684,\n",
       " 'drive': 685,\n",
       " 'merely': 686,\n",
       " 'tip': 687,\n",
       " 'obviously': 688,\n",
       " 'dark': 689,\n",
       " 'crawled': 690,\n",
       " 'darkness': 691,\n",
       " 'such': 692,\n",
       " 'waited': 693,\n",
       " 'empty': 694,\n",
       " 'nonetheless': 695,\n",
       " 'spent': 696,\n",
       " 'bear': 697,\n",
       " 'anxiously': 698,\n",
       " 'rather': 699,\n",
       " 'feet': 700,\n",
       " 'brought': 701,\n",
       " 'month': 702,\n",
       " 'breathe': 703,\n",
       " 'becoming': 704,\n",
       " 'dinner': 705,\n",
       " 'fetch': 706,\n",
       " 'decided': 707,\n",
       " 'appearance': 708,\n",
       " 'high': 709,\n",
       " 'stepped': 710,\n",
       " 'broke': 711,\n",
       " 'arrived': 712,\n",
       " 'coat': 713,\n",
       " 'pockets': 714,\n",
       " 'friends': 715,\n",
       " 'bedroom': 716,\n",
       " 'briefly': 717,\n",
       " 'corner': 718,\n",
       " 'rented': 719,\n",
       " 'rid': 720,\n",
       " 'horrible': 721,\n",
       " 'belly': 722,\n",
       " 'stiff': 723,\n",
       " 'proper': 724,\n",
       " 'peacefully': 725,\n",
       " 'samples': 726,\n",
       " 'recently': 727,\n",
       " 'showed': 728,\n",
       " 'rain': 729,\n",
       " 'sad': 730,\n",
       " 'rolled': 731,\n",
       " 'top': 732,\n",
       " 'bad': 733,\n",
       " 'lift': 734,\n",
       " 'cold': 735,\n",
       " 'slid': 736,\n",
       " 'salesmen': 737,\n",
       " 'sitting': 738,\n",
       " 'spot': 739,\n",
       " 'ago': 740,\n",
       " 'hearing': 741,\n",
       " 'debt': 742,\n",
       " 'six': 743,\n",
       " 'big': 744,\n",
       " 'true': 745,\n",
       " 'deeply': 746,\n",
       " 'lively': 747,\n",
       " 'avoid': 748,\n",
       " 'anger': 749,\n",
       " 'report': 750,\n",
       " 'understanding': 751,\n",
       " 'extremely': 752,\n",
       " 'suspicious': 753,\n",
       " 'yet': 754,\n",
       " 'company': 755,\n",
       " 'apart': 756,\n",
       " 'knock': 757,\n",
       " 'deep': 758,\n",
       " 'inside': 759,\n",
       " 'mixed': 760,\n",
       " 'properly': 761,\n",
       " 'explanation': 762,\n",
       " 'knocking': 763,\n",
       " 'opening': 764,\n",
       " 'consider': 765,\n",
       " 'remembered': 766,\n",
       " 'simple': 767,\n",
       " 'broad': 768,\n",
       " 'directions': 769,\n",
       " 'stretch': 770,\n",
       " 'managed': 771,\n",
       " 'weight': 772,\n",
       " 'occurred': 773,\n",
       " 'injured': 774,\n",
       " 'afraid': 775,\n",
       " 'watching': 776,\n",
       " 'bringing': 777,\n",
       " 'narrow': 778,\n",
       " 'lightly': 779,\n",
       " 'total': 780,\n",
       " 'ask': 781,\n",
       " 'falling': 782,\n",
       " 'raise': 783,\n",
       " 'forth': 784,\n",
       " 'caught': 785,\n",
       " 'firm': 786,\n",
       " 'spend': 787,\n",
       " 'upset': 788,\n",
       " \"clerk's\": 789,\n",
       " 'wants': 790,\n",
       " \"he'll\": 791,\n",
       " 'town': 792,\n",
       " \"you'll\": 793,\n",
       " 'explaining': 794,\n",
       " 'silence': 795,\n",
       " 'crying': 796,\n",
       " 'intention': 797,\n",
       " 'suitable': 798,\n",
       " 'behaviour': 799,\n",
       " 'causing': 800,\n",
       " 'employer': 801,\n",
       " 'immediate': 802,\n",
       " 'am': 803,\n",
       " 'giving': 804,\n",
       " 'wish': 805,\n",
       " 'nor': 806,\n",
       " 'waste': 807,\n",
       " 'learn': 808,\n",
       " 'sir': 809,\n",
       " 'shocking': 810,\n",
       " 'suffer': 811,\n",
       " 'sent': 812,\n",
       " 'calmly': 813,\n",
       " \"we're\": 814,\n",
       " 'contrast': 815,\n",
       " 'drawn': 816,\n",
       " 'meanwhile': 817,\n",
       " 'flowed': 818,\n",
       " 'following': 819,\n",
       " 'efforts': 820,\n",
       " 'double': 821,\n",
       " 'occupied': 822,\n",
       " 'hair': 823,\n",
       " 'grey': 824,\n",
       " 'line': 825,\n",
       " 'large': 826,\n",
       " 'remember': 827,\n",
       " 'usually': 828,\n",
       " 'partly': 829,\n",
       " 'shoulders': 830,\n",
       " 'secret': 831,\n",
       " 'sudden': 832,\n",
       " 'waiting': 833,\n",
       " 'convinced': 834,\n",
       " 'provide': 835,\n",
       " 'persuade': 836,\n",
       " 'understood': 837,\n",
       " 'landed': 838,\n",
       " 'outstretched': 839,\n",
       " 'sake': 840,\n",
       " 'coffee': 841,\n",
       " 'fled': 842,\n",
       " 'relatively': 843,\n",
       " 'picked': 844,\n",
       " 'appeals': 845,\n",
       " 'hissing': 846,\n",
       " 'distance': 847,\n",
       " 'further': 848,\n",
       " 'fast': 849,\n",
       " 'shove': 850,\n",
       " 'heavily': 851,\n",
       " 'feeling': 852,\n",
       " 'smell': 853,\n",
       " 'normally': 854,\n",
       " 'enter': 855,\n",
       " 'toe': 856,\n",
       " 'remain': 857,\n",
       " 'uneasy': 858,\n",
       " 'regret': 859,\n",
       " 'hunger': 860,\n",
       " 'greatest': 861,\n",
       " 'test': 862,\n",
       " 'watched': 863,\n",
       " 'realise': 864,\n",
       " 'bare': 865,\n",
       " 'cheese': 866,\n",
       " 'comfortable': 867,\n",
       " 'liked': 868,\n",
       " 'finger': 869,\n",
       " 'eaten': 870,\n",
       " 'midday': 871,\n",
       " 'news': 872,\n",
       " 'delay': 873,\n",
       " 'ate': 874,\n",
       " 'misfortune': 875,\n",
       " 'despair': 876,\n",
       " 'success': 877,\n",
       " 'earned': 878,\n",
       " 'warm': 879,\n",
       " 'fond': 880,\n",
       " 'conservatory': 881,\n",
       " 'christmas': 882,\n",
       " 'continue': 883,\n",
       " 'pull': 884,\n",
       " 'repeated': 885,\n",
       " 'lot': 886,\n",
       " 'interest': 887,\n",
       " 'nodded': 888,\n",
       " 'closer': 889,\n",
       " 'strain': 890,\n",
       " 'sofa': 891,\n",
       " 'child': 892,\n",
       " 'clothes': 893,\n",
       " 'experienced': 894,\n",
       " 'easier': 895,\n",
       " 'carrying': 896,\n",
       " 'improvement': 897,\n",
       " 'letting': 898,\n",
       " 'entertaining': 899,\n",
       " 'dare': 900,\n",
       " 'thrown': 901,\n",
       " \"we've\": 902,\n",
       " 'insist': 903,\n",
       " 'admit': 904,\n",
       " 'worn': 905,\n",
       " 'changed': 906,\n",
       " \"let's\": 907,\n",
       " 'chase': 908,\n",
       " \"grete's\": 909,\n",
       " 'screamed': 910,\n",
       " 'shaking': 911,\n",
       " 'death': 912,\n",
       " 'mean': 913,\n",
       " 'standing': 914,\n",
       " 'gold': 915,\n",
       " 'buttons': 916,\n",
       " 'cap': 917,\n",
       " 'movements': 918,\n",
       " 'contrary': 919,\n",
       " 'absolutely': 920,\n",
       " 'dust': 921,\n",
       " 'clean': 922,\n",
       " 'chairs': 923,\n",
       " 'play': 924,\n",
       " 'beards': 925,\n",
       " 'hurry': 926,\n",
       " 'smiled': 927,\n",
       " 'played': 928,\n",
       " 'hallway': 929,\n",
       " 'young': 930,\n",
       " 'neck': 931,\n",
       " 'dead': 932,\n",
       " 'daughter': 933,\n",
       " 'letters': 934,\n",
       " 'woke': 935,\n",
       " 'dreams': 936,\n",
       " 'thin': 937,\n",
       " 'size': 938,\n",
       " 'dream': 939,\n",
       " 'familiar': 940,\n",
       " 'collection': 941,\n",
       " 'salesman': 942,\n",
       " 'frame': 943,\n",
       " 'fitted': 944,\n",
       " 'dull': 945,\n",
       " 'weather': 946,\n",
       " 'drops': 947,\n",
       " 'hitting': 948,\n",
       " 'pane': 949,\n",
       " 'chosen': 950,\n",
       " 'curse': 951,\n",
       " 'worries': 952,\n",
       " 'itch': 953,\n",
       " 'makes': 954,\n",
       " 'stupid': 955,\n",
       " \"you've\": 956,\n",
       " 'instance': 957,\n",
       " 'house': 958,\n",
       " 'breakfasts': 959,\n",
       " \"he'd\": 960,\n",
       " 'definitely': 961,\n",
       " 'rung': 962,\n",
       " 'slept': 963,\n",
       " 'mad': 964,\n",
       " 'particularly': 965,\n",
       " \"boss's\": 966,\n",
       " 'assistant': 967,\n",
       " 'fifteen': 968,\n",
       " 'service': 969,\n",
       " 'accuse': 970,\n",
       " 'son': 971,\n",
       " 'hungrier': 972,\n",
       " 'cautious': 973,\n",
       " 'painful': 974,\n",
       " 'explain': 975,\n",
       " 'circumstances': 976,\n",
       " 'thank': 977,\n",
       " 'wooden': 978,\n",
       " 'members': 979,\n",
       " 'fist': 980,\n",
       " \"aren't\": 981,\n",
       " 'putting': 982,\n",
       " 'individual': 983,\n",
       " 'whispered': 984,\n",
       " 'beg': 985,\n",
       " 'acquired': 986,\n",
       " 'conclusions': 987,\n",
       " 'matter': 988,\n",
       " 'covers': 989,\n",
       " 'blow': 990,\n",
       " 'moreover': 991,\n",
       " 'bend': 992,\n",
       " 'imagine': 993,\n",
       " 'gather': 994,\n",
       " 'sensitive': 995,\n",
       " 'eventually': 996,\n",
       " 'miracle': 997,\n",
       " 'price': 998,\n",
       " 'sacrifice': 999,\n",
       " 'remind': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12edcd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2617\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(tokenizer.word_index)+1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805fadcc",
   "metadata": {},
   "source": [
    "Here we map one word-token to the next word-token as shown in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1968e773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of sequences are:  22046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  52,  139],\n",
       "       [ 139,   55],\n",
       "       [  55,   14],\n",
       "       [  14,   93],\n",
       "       [  93,  935],\n",
       "       [ 935,   28],\n",
       "       [  28, 1313],\n",
       "       [1313,  936],\n",
       "       [ 936,    4],\n",
       "       [   4,  241]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences=[]\n",
    "\n",
    "for i in range(1,len(sequence_data)):\n",
    "    words=sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The length of sequences are: \",len(sequences))\n",
    "sequences=np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d12aa3",
   "metadata": {},
   "source": [
    "Here we divide the sequences into X and y variabels.\n",
    "X being the current word-token \n",
    "y being the next word-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "42be41ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X=np.array(X)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "eea5b78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is:  [ 52 139  55  14  93]\n",
      "The responses are:  [139  55  14  93 935]\n"
     ]
    }
   ],
   "source": [
    "print(\"The data is: \",X[:5])\n",
    "print(\"The responses are: \",y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a257b8a",
   "metadata": {},
   "source": [
    "to_categorical converts the tokens into an array of 0s and 1s with some length.\n",
    "\n",
    "Example here we have a list(0,1,2,19) and num_classes=20.\n",
    "Therefore we will have an array of size 20 for each of the values in the list and\n",
    "all values in the array will be zero except the position of the items in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e65a7c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.keras.utils.to_categorical([0, 1, 2, 19], num_classes=20)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89886dde",
   "metadata": {},
   "source": [
    "We do the same here where our array size= vocab_size(2617) and the value is 1 for each of \n",
    "token values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2db4b4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=to_categorical(y,num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a682c5",
   "metadata": {},
   "source": [
    "## Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a4f617d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,10,input_length=1))\n",
    "model.add(LSTM(1000,return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000,activation=\"relu\"))\n",
    "model.add(Dense(vocab_size,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b732b885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 1, 10)             26170     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 1, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2617)              2619617   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15694787 (59.87 MB)\n",
      "Trainable params: 15694787 (59.87 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bd8528b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "\n",
    "reduce=ReduceLROnPlateau(monitor='loss',factor=0.2,patience=3,min_lr=0.0001,verbose=1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization=TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1dd2ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e92c3302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.8719\n",
      "Epoch 1: loss improved from 3.90922 to 3.87193, saving model to nextword1.h5\n",
      "345/345 [==============================] - 81s 233ms/step - loss: 3.8719 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.8361\n",
      "Epoch 2: loss improved from 3.87193 to 3.83605, saving model to nextword1.h5\n",
      "345/345 [==============================] - 80s 231ms/step - loss: 3.8361 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.8027\n",
      "Epoch 3: loss improved from 3.83605 to 3.80275, saving model to nextword1.h5\n",
      "345/345 [==============================] - 79s 229ms/step - loss: 3.8027 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.7732\n",
      "Epoch 4: loss improved from 3.80275 to 3.77324, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 227ms/step - loss: 3.7732 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.7446\n",
      "Epoch 5: loss improved from 3.77324 to 3.74461, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 226ms/step - loss: 3.7446 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.7169\n",
      "Epoch 6: loss improved from 3.74461 to 3.71695, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 225ms/step - loss: 3.7169 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.6903\n",
      "Epoch 7: loss improved from 3.71695 to 3.69029, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 225ms/step - loss: 3.6903 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.6645\n",
      "Epoch 8: loss improved from 3.69029 to 3.66445, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 226ms/step - loss: 3.6645 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.6381\n",
      "Epoch 9: loss improved from 3.66445 to 3.63813, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 226ms/step - loss: 3.6381 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.6197\n",
      "Epoch 10: loss improved from 3.63813 to 3.61972, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 226ms/step - loss: 3.6197 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.5920\n",
      "Epoch 11: loss improved from 3.61972 to 3.59198, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 226ms/step - loss: 3.5920 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.5836\n",
      "Epoch 12: loss improved from 3.59198 to 3.58356, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 225ms/step - loss: 3.5836 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.5576\n",
      "Epoch 13: loss improved from 3.58356 to 3.55765, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 225ms/step - loss: 3.5576 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.5423\n",
      "Epoch 14: loss improved from 3.55765 to 3.54225, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 225ms/step - loss: 3.5423 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.5218\n",
      "Epoch 15: loss improved from 3.54225 to 3.52179, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 227ms/step - loss: 3.5218 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.5052\n",
      "Epoch 16: loss improved from 3.52179 to 3.50517, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 226ms/step - loss: 3.5052 - lr: 0.0010\n",
      "Epoch 17/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.4940\n",
      "Epoch 17: loss improved from 3.50517 to 3.49405, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 227ms/step - loss: 3.4940 - lr: 0.0010\n",
      "Epoch 18/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.4798\n",
      "Epoch 18: loss improved from 3.49405 to 3.47985, saving model to nextword1.h5\n",
      "345/345 [==============================] - 79s 229ms/step - loss: 3.4798 - lr: 0.0010\n",
      "Epoch 19/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.4663\n",
      "Epoch 19: loss improved from 3.47985 to 3.46630, saving model to nextword1.h5\n",
      "345/345 [==============================] - 81s 234ms/step - loss: 3.4663 - lr: 0.0010\n",
      "Epoch 20/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.4520\n",
      "Epoch 20: loss improved from 3.46630 to 3.45196, saving model to nextword1.h5\n",
      "345/345 [==============================] - 80s 232ms/step - loss: 3.4520 - lr: 0.0010\n",
      "Epoch 21/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.4390\n",
      "Epoch 21: loss improved from 3.45196 to 3.43899, saving model to nextword1.h5\n",
      "345/345 [==============================] - 79s 230ms/step - loss: 3.4390 - lr: 0.0010\n",
      "Epoch 22/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.4294\n",
      "Epoch 22: loss improved from 3.43899 to 3.42937, saving model to nextword1.h5\n",
      "345/345 [==============================] - 80s 232ms/step - loss: 3.4294 - lr: 0.0010\n",
      "Epoch 23/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.4187\n",
      "Epoch 23: loss improved from 3.42937 to 3.41869, saving model to nextword1.h5\n",
      "345/345 [==============================] - 81s 233ms/step - loss: 3.4187 - lr: 0.0010\n",
      "Epoch 24/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.4093\n",
      "Epoch 24: loss improved from 3.41869 to 3.40929, saving model to nextword1.h5\n",
      "345/345 [==============================] - 81s 235ms/step - loss: 3.4093 - lr: 0.0010\n",
      "Epoch 25/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3956\n",
      "Epoch 25: loss improved from 3.40929 to 3.39559, saving model to nextword1.h5\n",
      "345/345 [==============================] - 80s 233ms/step - loss: 3.3956 - lr: 0.0010\n",
      "Epoch 26/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3894\n",
      "Epoch 26: loss improved from 3.39559 to 3.38942, saving model to nextword1.h5\n",
      "345/345 [==============================] - 81s 234ms/step - loss: 3.3894 - lr: 0.0010\n",
      "Epoch 27/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3794\n",
      "Epoch 27: loss improved from 3.38942 to 3.37945, saving model to nextword1.h5\n",
      "345/345 [==============================] - 79s 230ms/step - loss: 3.3794 - lr: 0.0010\n",
      "Epoch 28/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3732\n",
      "Epoch 28: loss improved from 3.37945 to 3.37315, saving model to nextword1.h5\n",
      "345/345 [==============================] - 79s 230ms/step - loss: 3.3732 - lr: 0.0010\n",
      "Epoch 29/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3630\n",
      "Epoch 29: loss improved from 3.37315 to 3.36296, saving model to nextword1.h5\n",
      "345/345 [==============================] - 79s 229ms/step - loss: 3.3630 - lr: 0.0010\n",
      "Epoch 30/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3548\n",
      "Epoch 30: loss improved from 3.36296 to 3.35476, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 225ms/step - loss: 3.3548 - lr: 0.0010\n",
      "Epoch 31/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3472\n",
      "Epoch 31: loss improved from 3.35476 to 3.34724, saving model to nextword1.h5\n",
      "345/345 [==============================] - 77s 225ms/step - loss: 3.3472 - lr: 0.0010\n",
      "Epoch 32/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3416\n",
      "Epoch 32: loss improved from 3.34724 to 3.34159, saving model to nextword1.h5\n",
      "345/345 [==============================] - 79s 230ms/step - loss: 3.3416 - lr: 0.0010\n",
      "Epoch 33/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3371\n",
      "Epoch 33: loss improved from 3.34159 to 3.33706, saving model to nextword1.h5\n",
      "345/345 [==============================] - 80s 232ms/step - loss: 3.3371 - lr: 0.0010\n",
      "Epoch 34/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3316\n",
      "Epoch 34: loss improved from 3.33706 to 3.33157, saving model to nextword1.h5\n",
      "345/345 [==============================] - 80s 233ms/step - loss: 3.3316 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3234\n",
      "Epoch 35: loss improved from 3.33157 to 3.32342, saving model to nextword1.h5\n",
      "345/345 [==============================] - 80s 231ms/step - loss: 3.3234 - lr: 0.0010\n",
      "Epoch 36/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3137\n",
      "Epoch 36: loss improved from 3.32342 to 3.31368, saving model to nextword1.h5\n",
      "345/345 [==============================] - 79s 230ms/step - loss: 3.3137 - lr: 0.0010\n",
      "Epoch 37/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3112\n",
      "Epoch 37: loss improved from 3.31368 to 3.31115, saving model to nextword1.h5\n",
      "345/345 [==============================] - 80s 232ms/step - loss: 3.3112 - lr: 0.0010\n",
      "Epoch 38/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.3015\n",
      "Epoch 38: loss improved from 3.31115 to 3.30154, saving model to nextword1.h5\n",
      "345/345 [==============================] - 80s 231ms/step - loss: 3.3015 - lr: 0.0010\n",
      "Epoch 39/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.2977\n",
      "Epoch 39: loss improved from 3.30154 to 3.29768, saving model to nextword1.h5\n",
      "345/345 [==============================] - 80s 232ms/step - loss: 3.2977 - lr: 0.0010\n",
      "Epoch 40/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.2903\n",
      "Epoch 40: loss improved from 3.29768 to 3.29026, saving model to nextword1.h5\n",
      "345/345 [==============================] - 79s 230ms/step - loss: 3.2903 - lr: 0.0010\n",
      "Epoch 41/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.2883\n",
      "Epoch 41: loss improved from 3.29026 to 3.28826, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 227ms/step - loss: 3.2883 - lr: 0.0010\n",
      "Epoch 42/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.2859\n",
      "Epoch 42: loss improved from 3.28826 to 3.28594, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 226ms/step - loss: 3.2859 - lr: 0.0010\n",
      "Epoch 43/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.2772\n",
      "Epoch 43: loss improved from 3.28594 to 3.27719, saving model to nextword1.h5\n",
      "345/345 [==============================] - 77s 224ms/step - loss: 3.2772 - lr: 0.0010\n",
      "Epoch 44/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.2739\n",
      "Epoch 44: loss improved from 3.27719 to 3.27391, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 225ms/step - loss: 3.2739 - lr: 0.0010\n",
      "Epoch 45/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.2706\n",
      "Epoch 45: loss improved from 3.27391 to 3.27064, saving model to nextword1.h5\n",
      "345/345 [==============================] - 77s 224ms/step - loss: 3.2706 - lr: 0.0010\n",
      "Epoch 46/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.2655\n",
      "Epoch 46: loss improved from 3.27064 to 3.26554, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 225ms/step - loss: 3.2655 - lr: 0.0010\n",
      "Epoch 47/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.2607\n",
      "Epoch 47: loss improved from 3.26554 to 3.26066, saving model to nextword1.h5\n",
      "345/345 [==============================] - 77s 224ms/step - loss: 3.2607 - lr: 0.0010\n",
      "Epoch 48/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.2597\n",
      "Epoch 48: loss improved from 3.26066 to 3.25974, saving model to nextword1.h5\n",
      "345/345 [==============================] - 77s 225ms/step - loss: 3.2597 - lr: 0.0010\n",
      "Epoch 49/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.2524\n",
      "Epoch 49: loss improved from 3.25974 to 3.25240, saving model to nextword1.h5\n",
      "345/345 [==============================] - 77s 224ms/step - loss: 3.2524 - lr: 0.0010\n",
      "Epoch 50/50\n",
      "345/345 [==============================] - ETA: 0s - loss: 3.2484\n",
      "Epoch 50: loss improved from 3.25240 to 3.24836, saving model to nextword1.h5\n",
      "345/345 [==============================] - 78s 225ms/step - loss: 3.2484 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dd417394e0>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce656b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c70d1d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=' what a strenuous'\n",
    "text=text.split(\" \")\n",
    "text=text[-1]\n",
    "text=''.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7709c7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "sequence = np.array(sequence)\n",
    "        \n",
    "preds = model.predict(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2ab25fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 5.1278437e-10, 1.5114620e-11, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "73385d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_word = \"\"\n",
    "\n",
    "for key, value in tokenizer.word_index.items():\n",
    "    if np.array_equal(value, preds):\n",
    "        predicted_word = key\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2cf8fa7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "88671def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 5.1278437e-10 1.5114620e-11 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "633e8492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0000000e+00 5.1278437e-10 1.5114620e-11 ... 0.0000000e+00 0.0000000e+00\n",
      " 0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "for i in preds:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d11adff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 5.1278437e-10, 1.5114620e-11, ..., 0.0000000e+00,\n",
       "       0.0000000e+00, 0.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "04352a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.texts_to_sequences([text])[0]\n",
    "# sequence = np.array(sequence)\n",
    "        \n",
    "# preds = model.predict(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1e6da3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1340]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e877dc46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1340])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d2a1e91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 5.1278437e-10, 1.5114620e-11, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5bafddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_labels = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "395de037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1341], dtype=int64)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "50330eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,value in tokenizer.word_index.items():\n",
    "    if value==decoded_labels:\n",
    "        predi=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "149ee290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'career'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22e916",
   "metadata": {},
   "source": [
    "Project Reference- Bharath-K3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f4a957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
